#!/usr/bin/env python3
"""
Reddit ê¸ˆìœµ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ Top/Day ê²Œì‹œë¬¼ ìˆ˜ì§‘
"""

import praw
import json
import os
from datetime import datetime, timedelta
import logging
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Reddit ì„¤ì •
REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')
REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')
REDDIT_USER_AGENT = os.getenv('REDDIT_USER_AGENT', 'InsightPipeline/1.0')

# ê²€ìƒ‰í•  í‚¤ì›Œë“œë“¤
KEYWORDS = ['S&P500', 'KOSPI', 'FOMC', 'AI', 'Bitcoin', 'Tesla', 'Apple', 'NVIDIA']

def init_reddit():
    """Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    try:
        reddit = praw.Reddit(
            client_id=REDDIT_CLIENT_ID,
            client_secret=REDDIT_CLIENT_SECRET,
            user_agent=REDDIT_USER_AGENT
        )
        return reddit
    except Exception as e:
        logger.error(f"âŒ Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None

def search_reddit_posts(reddit, keyword, limit=5):
    """Redditì—ì„œ í‚¤ì›Œë“œë¡œ ê²Œì‹œë¬¼ ê²€ìƒ‰"""
    posts = []
    
    try:
        # ì¸ê¸° ì„œë¸Œë ˆë”§ë“¤ì—ì„œ ê²€ìƒ‰
        subreddits = ['investing', 'stocks', 'wallstreetbets', 'cryptocurrency', 'economics']
        
        for subreddit_name in subreddits:
            try:
                subreddit = reddit.subreddit(subreddit_name)
                
                # Top ê²Œì‹œë¬¼ ê²€ìƒ‰
                top_posts = subreddit.search(keyword, sort='top', time_filter='day', limit=limit)
                
                for post in top_posts:
                    # ìŠ¤ì½”ì–´ ê³„ì‚° (ì—…ë³´íŠ¸ + ëŒ“ê¸€ ìˆ˜ + ì‹œê°„ ê°€ì¤‘ì¹˜)
                    hours_ago = (datetime.now() - datetime.fromtimestamp(post.created_utc)).total_seconds() / 3600
                    time_weight = max(0.1, 1 - (hours_ago / 24))  # 24ì‹œê°„ ë‚´ ê°€ì¤‘ì¹˜
                    score = (post.score + post.num_comments) * time_weight
                    
                    posts.append({
                        'id': post.id,
                        'title': post.title,
                        'url': f"https://reddit.com{post.permalink}",
                        'subreddit': subreddit_name,
                        'score': post.score,
                        'num_comments': post.num_comments,
                        'created_utc': post.created_utc,
                        'author': str(post.author) if post.author else '[deleted]',
                        'selftext': post.selftext[:500] if post.selftext else '',
                        'calculated_score': score,
                        'keyword': keyword
                    })
                    
            except Exception as e:
                logger.warning(f"âš ï¸ {subreddit_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue
                
    except Exception as e:
        logger.error(f"âŒ Reddit ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    return posts

def fetch_reddit_data():
    """Reddit ë°ì´í„° ìˆ˜ì§‘"""
    reddit = init_reddit()
    if not reddit:
        logger.warning("âš ï¸ Reddit í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return generate_dummy_reddit_data()
    
    all_posts = []
    
    # API í…ŒìŠ¤íŠ¸
    try:
        test_subreddit = reddit.subreddit('test')
        test_posts = list(test_subreddit.hot(limit=1))
        if not test_posts:
            logger.warning("âš ï¸ Reddit API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return generate_dummy_reddit_data()
    except Exception as e:
        logger.warning(f"âš ï¸ Reddit API í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return generate_dummy_reddit_data()
    
    for keyword in KEYWORDS:
        logger.info(f"ğŸ” '{keyword}' í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì¤‘...")
        posts = search_reddit_posts(reddit, keyword, limit=3)
        all_posts.extend(posts)
    
    # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    all_posts.sort(key=lambda x: x['calculated_score'], reverse=True)
    
    if not all_posts:
        logger.warning("âš ï¸ Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨. ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return generate_dummy_reddit_data()
    
    # ìƒìœ„ 15ê°œë§Œ ì„ íƒ
    top_posts = all_posts[:15]
    
    reddit_data = {
        'timestamp': datetime.now().isoformat(),
        'posts': top_posts,
        'total_posts': len(all_posts),
        'keywords_searched': KEYWORDS
    }
    
    return reddit_data

def generate_dummy_reddit_data():
    """ë”ë¯¸ Reddit ë°ì´í„° ìƒì„±"""
    logger.info("ğŸ“Š Reddit ë”ë¯¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    dummy_posts = [
        {
            'id': 'dummy1',
            'title': 'S&P 500 hits new all-time high as tech stocks rally',
            'url': 'https://reddit.com/r/investing/comments/dummy1',
            'subreddit': 'investing',
            'score': 1250,
            'num_comments': 89,
            'created_utc': datetime.now().timestamp() - 3600,
            'author': 'market_watcher',
            'selftext': 'Tech stocks continue their strong performance with AI and semiconductor companies leading the charge.',
            'calculated_score': 1250,
            'keyword': 'S&P500'
        },
        {
            'id': 'dummy2',
            'title': 'Tesla Q4 earnings beat expectations, stock up 5%',
            'url': 'https://reddit.com/r/stocks/comments/dummy2',
            'subreddit': 'stocks',
            'score': 890,
            'num_comments': 156,
            'created_utc': datetime.now().timestamp() - 7200,
            'author': 'tesla_bull',
            'selftext': 'Tesla reported strong Q4 results with record deliveries and improved margins.',
            'calculated_score': 890,
            'keyword': 'Tesla'
        },
        {
            'id': 'dummy3',
            'title': 'FOMC meeting this week - what to expect',
            'url': 'https://reddit.com/r/economics/comments/dummy3',
            'subreddit': 'economics',
            'score': 567,
            'num_comments': 234,
            'created_utc': datetime.now().timestamp() - 10800,
            'author': 'fed_watcher',
            'selftext': 'Federal Reserve meeting this week could provide clues about future rate cuts.',
            'calculated_score': 567,
            'keyword': 'FOMC'
        },
        {
            'id': 'dummy4',
            'title': 'Bitcoin breaks $50k resistance level',
            'url': 'https://reddit.com/r/cryptocurrency/comments/dummy4',
            'subreddit': 'cryptocurrency',
            'score': 2340,
            'num_comments': 445,
            'created_utc': datetime.now().timestamp() - 14400,
            'author': 'crypto_analyst',
            'selftext': 'Bitcoin successfully broke through the $50k resistance level with strong volume.',
            'calculated_score': 2340,
            'keyword': 'Bitcoin'
        },
        {
            'id': 'dummy5',
            'title': 'NVIDIA AI chips demand continues to surge',
            'url': 'https://reddit.com/r/stocks/comments/dummy5',
            'subreddit': 'stocks',
            'score': 678,
            'num_comments': 123,
            'created_utc': datetime.now().timestamp() - 18000,
            'author': 'tech_investor',
            'selftext': 'NVIDIA continues to dominate the AI chip market with strong demand from data centers.',
            'calculated_score': 678,
            'keyword': 'NVIDIA'
        }
    ]
    
    reddit_data = {
        'timestamp': datetime.now().isoformat(),
        'posts': dummy_posts,
        'total_posts': len(dummy_posts),
        'keywords_searched': KEYWORDS
    }
    
    logger.info(f"âœ… Reddit ë”ë¯¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(dummy_posts)}ê°œ ê²Œì‹œë¬¼")
    return reddit_data

def save_data(data, date_str):
    """ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(f'data/{date_str}', exist_ok=True)
    filepath = f'data/{date_str}/reddit.json'
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥: {filepath}")
    return filepath

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ”´ Reddit ê¸ˆìœµ ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì‹œì‘")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ
    today = datetime.now().strftime('%Y-%m-%d')
    
    # Reddit ë°ì´í„° ìˆ˜ì§‘
    reddit_data = fetch_reddit_data()
    
    if reddit_data:
        # ë°ì´í„° ì €ì¥
        filepath = save_data(reddit_data, today)
        logger.info(f"âœ… Reddit ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(reddit_data['posts'])}ê°œ ê²Œì‹œë¬¼")
    else:
        logger.error("âŒ Reddit ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
    
    return filepath if reddit_data else None

if __name__ == "__main__":
    main() 