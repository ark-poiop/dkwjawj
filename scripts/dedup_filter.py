#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ë°ì´í„° ì¤‘ë³µ ì œê±° ë° í•„í„°ë§ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì‚¬ ê¸¸ì´, ì¤‘ë³µ ì œê±°, ìŠ¤ì½”ì–´ ê³„ì‚°
"""

import json
import os
from datetime import datetime
import logging
from difflib import SequenceMatcher
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_similarity(text1, text2):
    """ë‘ í…ìŠ¤íŠ¸ ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚°"""
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
    text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    # ì—°ì† ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def filter_articles(articles, min_length=200, similarity_threshold=0.8):
    """ê¸°ì‚¬ í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°"""
    filtered_articles = []
    
    for article in articles:
        # í…ìŠ¤íŠ¸ ì •ì œ
        title = clean_text(article.get('title', ''))
        content = clean_text(article.get('content', ''))
        
        # ê¸¸ì´ ì²´í¬
        if len(title + content) < min_length:
            continue
        
        # ì¤‘ë³µ ì²´í¬
        is_duplicate = False
        for existing in filtered_articles:
            existing_title = clean_text(existing.get('title', ''))
            existing_content = clean_text(existing.get('content', ''))
            
            title_similarity = calculate_similarity(title, existing_title)
            content_similarity = calculate_similarity(content, existing_content)
            
            if title_similarity > similarity_threshold or content_similarity > similarity_threshold:
                is_duplicate = True
                break
        
        if not is_duplicate:
            # ìŠ¤ì½”ì–´ ê³„ì‚°
            score = calculate_article_score(article)
            article['cleaned_title'] = title
            article['cleaned_content'] = content
            article['filtered_score'] = score
            filtered_articles.append(article)
    
    return filtered_articles

def calculate_article_score(article):
    """ê¸°ì‚¬ ìŠ¤ì½”ì–´ ê³„ì‚°"""
    score = 0
    
    # ê¸°ë³¸ ì ìˆ˜
    score += 10
    
    # ì œëª© ê¸¸ì´ ê°€ì¤‘ì¹˜
    title_length = len(article.get('title', ''))
    if 20 <= title_length <= 100:
        score += 5
    elif title_length > 100:
        score += 2
    
    # ë‚´ìš© ê¸¸ì´ ê°€ì¤‘ì¹˜
    content_length = len(article.get('content', ''))
    if 200 <= content_length <= 1000:
        score += 10
    elif content_length > 1000:
        score += 15
    
    # í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
    keywords = ['S&P500', 'KOSPI', 'FOMC', 'AI', 'Bitcoin', 'Tesla', 'Apple', 'NVIDIA']
    title_lower = article.get('title', '').lower()
    content_lower = article.get('content', '').lower()
    
    for keyword in keywords:
        if keyword.lower() in title_lower:
            score += 3
        if keyword.lower() in content_lower:
            score += 1
    
    # ì‹œê°„ ê°€ì¤‘ì¹˜ (ìµœì‹  ê¸°ì‚¬ ìš°ì„ )
    if 'published_at' in article:
        try:
            published_time = datetime.fromisoformat(article['published_at'].replace('Z', '+00:00'))
            hours_ago = (datetime.now() - published_time).total_seconds() / 3600
            time_weight = max(0.1, 1 - (hours_ago / 24))  # 24ì‹œê°„ ë‚´ ê°€ì¤‘ì¹˜
            score *= time_weight
        except:
            pass
    
    return round(score, 2)

def process_reddit_data(reddit_file):
    """Reddit ë°ì´í„°ë¥¼ ë‰´ìŠ¤ í˜•íƒœë¡œ ë³€í™˜"""
    try:
        with open(reddit_file, 'r', encoding='utf-8') as f:
            reddit_data = json.load(f)
        
        articles = []
        for post in reddit_data.get('posts', []):
            article = {
                'title': post.get('title', ''),
                'content': post.get('selftext', ''),
                'source': f"Reddit r/{post.get('subreddit', '')}",
                'url': post.get('url', ''),
                'score': post.get('score', 0),
                'comments': post.get('num_comments', 0),
                'published_at': datetime.fromtimestamp(post.get('created_utc', 0)).isoformat(),
                'author': post.get('author', ''),
                'keyword': post.get('keyword', '')
            }
            articles.append(article)
        
        return articles
        
    except Exception as e:
        logger.error(f"âŒ Reddit ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

def save_clean_data(articles, date_str):
    """ì •ì œëœ ë°ì´í„° ì €ì¥"""
    os.makedirs(f'data/{date_str}', exist_ok=True)
    filepath = f'data/{date_str}/clean_news.json'
    
    clean_data = {
        'timestamp': datetime.now().isoformat(),
        'articles': articles,
        'total_articles': len(articles)
    }
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(clean_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ’¾ ì •ì œëœ ë°ì´í„° ì €ì¥: {filepath}")
    return filepath

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸ” ë‰´ìŠ¤ ë°ì´í„° ì •ì œ ë° í•„í„°ë§ ì‹œì‘")
    
    # ì˜¤ëŠ˜ ë‚ ì§œ
    today = datetime.now().strftime('%Y-%m-%d')
    reddit_file = f'data/{today}/reddit.json'
    
    if not os.path.exists(reddit_file):
        logger.error(f"âŒ Reddit ë°ì´í„° íŒŒì¼ ì—†ìŒ: {reddit_file}")
        return None
    
    # Reddit ë°ì´í„°ë¥¼ ë‰´ìŠ¤ í˜•íƒœë¡œ ë³€í™˜
    articles = process_reddit_data(reddit_file)
    
    if not articles:
        logger.error("âŒ ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤")
        return None
    
    logger.info(f"ğŸ“° ì´ {len(articles)}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì‹œì‘")
    
    # í•„í„°ë§ ë° ì¤‘ë³µ ì œê±°
    filtered_articles = filter_articles(articles)
    
    # ìŠ¤ì½”ì–´ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    filtered_articles.sort(key=lambda x: x['filtered_score'], reverse=True)
    
    # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
    top_articles = filtered_articles[:5]
    
    logger.info(f"âœ… í•„í„°ë§ ì™„ë£Œ: {len(top_articles)}ê°œ ê¸°ì‚¬ ì„ íƒ")
    
    # ì •ì œëœ ë°ì´í„° ì €ì¥
    filepath = save_clean_data(top_articles, today)
    
    return filepath

if __name__ == "__main__":
    main() 